{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_LHc97BTxVf",
        "outputId": "05cdf69c-f2a9-4996-f337-610977f962d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SP-lJ-GkvM2u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision.models.vgg import VGG16_Weights\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from os import listdir\n",
        "from os.path import isfile, join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zQHWw6sZZ8mi"
      },
      "outputs": [],
      "source": [
        "class CustomTrainDataset(Dataset):\n",
        "    def __init__(self, annotations_file, image_directory, transform=None, label_encoder=None , split='train', test_size=0.2, random_state=None):\n",
        "        self.labels = pd.read_csv(annotations_file)\n",
        "        self.image_directory = image_directory\n",
        "        self.transform = transform\n",
        "        self.label_encoder = label_encoder\n",
        "        train_labels, val_labels = train_test_split(self.labels, test_size=test_size, random_state=random_state)\n",
        "        if split == 'train':\n",
        "            self.labels = train_labels\n",
        "        elif split == 'val':\n",
        "            self.labels = val_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      image_path = os.path.join(self.image_directory, self.labels.iloc[idx]['File Name'])\n",
        "      image = Image.open(image_path).convert(\"RGB\")\n",
        "      if self.transform:\n",
        "          image = self.transform(image)\n",
        "      label = self.labels.iloc[idx]['Category']\n",
        "      if self.label_encoder:\n",
        "          label = self.label_encoder.transform([label])[0]\n",
        "      return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sRYEkgRuP12V"
      },
      "outputs": [],
      "source": [
        "annotations_file = '/content/drive/MyDrive/train.csv'\n",
        "image_directory = '/content/drive/MyDrive/minichallenge_train/train_preprocessed'\n",
        "\n",
        "## Use of label encoder for classification: Idea from https://discuss.pytorch.org/t/how-to-encode-labels-for-classification-on-custom-dataset/142396\n",
        "labels_df = pd.read_csv(annotations_file)\n",
        "labels = labels_df['Category'].tolist()\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "##Different data augmentations: Idea from https://colab.research.google.com/drive/109vu3F1LTzD1gdVV6cho9fKGx7lzbFll\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = CustomTrainDataset(annotations_file=annotations_file, image_directory= image_directory, transform=train_transforms, label_encoder=label_encoder, split='train', test_size=0.2, random_state=42)\n",
        "val_dataset = CustomTrainDataset(annotations_file=annotations_file, image_directory= image_directory, transform=val_transforms, label_encoder=label_encoder, split='val', test_size=0.2, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=12, pin_memory=True, prefetch_factor=36)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=12, pin_memory=True, prefetch_factor=36)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dUc2Tal5JJBM"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
        "\n",
        "## Fitting a pre-trained classifier with custom number of classes, idea from https://github.com/pytorch/vision/issues/3547\n",
        "num_classes = len(label_encoder.classes_)\n",
        "num_features = model.classifier[6].in_features\n",
        "model.classifier[6] = nn.Linear(num_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "## Basing the Cross Entropy Loss on balanced class weights, idea from https://stackoverflow.com/questions/69783897/compute-class-weight-function-issue-in-sklearn-library-when-used-in-keras-cl\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(encoded_labels), y=encoded_labels)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))\n",
        "\n",
        "## Using ReduceLROnPlateau, idea from https://discuss.pytorch.org/t/reduce-lr-on-plateau-based-on-training-loss-or-validation/183344\n",
        "optimizer = torch.optim.AdamW([{'params': model.features.parameters(), 'lr': 0.0, 'frozen': True}, {'params': model.classifier.parameters(), 'lr': 0.001, 'frozen': False}], weight_decay=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xszly2NUtKsJ",
        "outputId": "a3e8a1dd-263d-4543-845c-c2a1434f2f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/200, Train Loss: 4.3195:  60%|█████▉    | 130/218 [19:26<01:03,  1.39it/s]"
          ]
        }
      ],
      "source": [
        "best_val_loss_so_far = 100000\n",
        "patience = 50\n",
        "epoch_with_no_progress = 0\n",
        "num_epochs = 200\n",
        "initial_lr = 0.001\n",
        "last_lr = None\n",
        "\n",
        "unfreezing_schedule = {5: 4, 10: 8, 15: 12, 20: 'all'}\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Unfreezing VGG16 layers gradually: Idea from https://discuss.huggingface.co/t/gradual-layer-freezing/3381/4\n",
        "    if epoch in unfreezing_schedule.keys():\n",
        "        unfreezing_layers = unfreezing_schedule[epoch]\n",
        "        if unfreezing_layers == 'all':\n",
        "            for param in model.features.parameters():\n",
        "                param.requires_grad = True\n",
        "            for param_group in optimizer.param_groups:\n",
        "                if param_group.get('frozen', False):\n",
        "                    param_group['lr'] = initial_lr / 10\n",
        "                    param_group.pop('frozen', None)\n",
        "        else:\n",
        "            layer_list = list(model.features.children())\n",
        "            for layer in layer_list[-unfreezing_layers:]:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = True\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    if 'frozen' in param_group and param_group['frozen'] == True:\n",
        "                      param_group['lr'] = initial_lr / 10\n",
        "                      param_group.pop('frozen', None)\n",
        "    # Training loop: Idea of tqdm as a progress bar from https://adamoudad.github.io/posts/progress_bar_with_tqdm/\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\")\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        train_loader.set_description(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss / (batch_idx + 1):.4f}\")\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch: {epoch+1}/{num_epochs} , Train Loss: {train_loss:.6f}')\n",
        "\n",
        "    # Validation loop: Idea of validation loop structure from: https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        val_loader = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Validation\")\n",
        "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            val_loader.set_description(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss / (batch_idx + 1):.4f}\")\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100. * correct / total\n",
        "    print(f'Epoch: {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.6f}, Validation Acc: {val_acc:.2f}%')\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping: Idea from https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
        "    if val_loss < best_val_loss_so_far:\n",
        "        print(f'Got a new model that gives the best validation loss so far, saving to Drive!')\n",
        "        torch.save(model.state_dict(), \"drive/MyDrive/best_model_so_far.pth\")\n",
        "        best_val_loss_so_far = val_loss\n",
        "        epoch_with_no_progress = 0\n",
        "    else:\n",
        "        epoch_with_no_progress += 1\n",
        "        if epoch_with_no_progress >= patience:\n",
        "            print(\"Early stopping triggered due to no progress over validation loss!\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UEceBjT_hPe"
      },
      "outputs": [],
      "source": [
        "class CustomTestDataset(Dataset):\n",
        "    def __init__(self, image_directory, transform=None):\n",
        "        self.image_directory = image_directory\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(image_directory) if os.path.isfile(os.path.join(image_directory, f))]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_directory, self.image_files[idx])\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, self.image_files[idx]\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_dataset = CustomTestDataset(image_directory='drive/MyDrive/test_preprocessed', transform=test_transform)\n",
        "\n",
        "test_loader = DataLoaderval_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=12,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_ISMtcv_igK"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "file_names_list = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, file_names in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        file_names_list.extend(file_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar3ehAKa_jZ7"
      },
      "outputs": [],
      "source": [
        "predicted_labels = label_encoder.inverse_transform(predictions)\n",
        "indices = [int(f.split('.')[0]) for f in file_names_list]\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'Id': indices,\n",
        "    'Category': predicted_labels\n",
        "})\n",
        "\n",
        "results_df = results_df.sort_values(by='Id').reset_index(drop=True)\n",
        "results_df.to_csv('drive/MyDrive/test_predictions.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}